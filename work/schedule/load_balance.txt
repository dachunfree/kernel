		https://zhuanlan.zhihu.com/p/163482863?utm_source=wechat_session&utm_medium=social&utm_oi=730167171128233984
		在Linux SMP（对称多处理器）环境下，每个CPU对应一个run_queue（可执行队列）。如果一个进程处于TASK_RUNNING状态（可执行状态），
则它会被加入到其中一个run_queue（且同一时刻仅会被加入到一个run_queue），以便让调度程序安排它在这个run_queue对应的CPU上面运行。
	  一个CPU对应一个run_queue这样的设计，其好处是：
1、一个持续处于TASK_RUNNING状态的进程总是趋于在同一个CPU上面运行（其间，这个进程可能被抢占、然后又被调度），这有利于进程的数据被
	CPU所缓存，提高运行效率；
2、各个CPU上的调度程序只访问自己的run_queue，避免了竞争；
load_balance所需要做的事情就是，在一定的时机，通过将进程从一个run_queue迁移到另一个run_queue，来保持CPU之间的负载均衡。

普通进程的负载均衡在以下情况下会被触发：
1、当前进程离开TASK_RUNNING状态（进入睡眠或退出），而对应的run_queue中已无进程可用时。这时触发负载均衡，试图从别的run_queue
	中pull一个进程过来运行；
2、每隔一定的时间，启动负载均衡过程，试图发现并解决系统中不均衡；

load_balance的过程大致如下：
1、找出最繁忙的一个run_queue；
2、如果找到的run_queue比本地run_queue繁忙，且本地run_queue的繁忙程度低于平均水平，那么迁移几个进程过来，使两个run_queue的
		load接近平均水平。反之则什么都不做；

		在比较两个run_queue繁忙程度的问题上，其实是很有讲究的。这个地方很容易想当然地理解为：把run_queue中所有进程的load加起来，比较一下就OK了。
而实际上，需要比较的往往并不是实时的load。这就好比我们用top命令查看CPU占用率一样，top命令默认1秒刷新一次，每次刷新你将看到这1秒内所有进
程各自对CPU的占用情况。这里的占用率是个统计值，假设有一个进程在这1秒内持续运行了100毫秒，那么我们认为它占用了10%的CPU。如果把1秒刷新一次
改成1毫秒刷新一次呢？那么我们将有90%的机率看到这个进程占用0%的CPU、10%的机率占用100%的CPU。而无论是0%、还是100%，都不是这个进程真实的CPU
占用率的体现。必须把一段时间以内的CPU占用率综合起来看，才能得到我们需要的那个值。run_queue的load值也是这样。有些进程可能频繁地在
TASK_RUNNING和非TASK_RUNNING状态之间变换，导致run_queue的load值不断抖动。光看某一时刻的load值，我们是体会不到run_queue的负载情况的，
必须将一段时间内的load值综合起来看才行。于是，run_queue结构中维护了一个保存load值的数组：
unsigned long cpu_load[CPU_LOAD_IDX_MAX] （目前CPU_LOAD_IDX_MAX值为5）
每个CPU上，每个tick的时钟中断会调用到update_cpu_load函数，来更新该CPU所对应的run_queue的cpu_load值。
		cpu_load[i] = old_load + (new_load - old_load) / 2^i。i值越大，cpu_load[i]受load的实时值的影响越小，代表着越长时间内的平均负载情况。
而cpu_load[0]就是实时的load。
		尽管我们需要的是一段时间内的综合的负载情况，但是，为什么不是保存一个最合适的统计值，而要保存这么多的值呢？这是为了便于在不同场景下选择
不同的load。如果希望进行进程迁移，那么应该选择较小的i值，因为此时的cpu_load[i]抖动比较大，容易发现不均衡；反之，如果希望保持稳定，那么应该
选择较大的i值。
		什么时候倾向于进行迁移、什么时候又倾向于保持稳定呢？这要从两个维度来看：
		第一个维度，是当前CPU的状态。这里会考虑三种CPU状态：
1、CPU刚进入IDLE（比如说CPU上唯一的TASK_RUNNING状态的进程睡眠去了），这时候是很渴望马上弄一个进程过来运行的，应该选择较小的i值；
2、CPU处于IDLE，这时候还是很渴望弄一个进程过来运行的，但是可能已经尝试过几次都无果了，故选择略大一点的i值；
3、CPU非IDLE，有进程正在运行，这时候就不太希望进程迁移了，会选择较大的i值；
		第二个维度，是CPU的亲缘性。离得越近的CPU，进程迁移所造成的缓存失效的影响越小，应该选择较小的i值。比如两个CPU是同一物理CPU的同一
核心通过SMT（超线程技术）虚拟出来的，那么它们的缓存大部分是共享的。进程在它们之间迁移代价较小。反之则应该选择较大的i值。
（后面将会看到linux通过调度域来管理CPU的亲缘性）。

调度域

前面已经多次提到了调度域（sched_domain）。在复杂的SMP系统中，为了描述CPU与CPU之间的亲缘关系，引入了调度域。

两个CPU之间的亲缘关系主要有以下几种：

1、超线程。超线程CPU是一个可以“同时”执行几个线程的CPU。就像操作系统通过进程调度能够让多个进程“同时”在一个CPU上运行一样，超线程
	 CPU也是通过这样的分时复用技术来实现几个线程的“同时”执行的。这样做之所以能够提高执行效率，是因为CPU的速度比内存速度快很多
	 （一个数量级以上）。如果cache不能命中，CPU在等待内存的时间内将无事可做，可以切换到其他线程去执行。这样的多个线程对于操作系统
	 来说就相当于多个CPU，它们共享着大部分的cache，非常之亲近；

2、同一物理CPU上的不同核心。现在的多核CPU大多属于这种情况，每个CPU核心都有独立执行程序的能力，而它们之间也会共享着一些cache；

3、同一NUMA结点上的CPU；

4、不同NUMA结点上的CPU；

普通进程的load_balance第一步是需要找出一个最繁忙的CPU，实际上这是通过两个步骤来实现的：

1、找出sched_domain下最繁忙的一个sched_group（组内的CPU对应的run_queue的load之和最高）；

2、从该sched_group下找出最繁忙的CPU
可见，load_balance实际上是实现了对应sched_domain下的sched_group之间的平衡。较高层次的sched_domain包含了很多CPU，但是在这个
sched_domain上的load_balance并不直接解决这些CPU之间的负载均衡，而只是解决sched_group之间的平衡（这又是load_balance的一大简化）。
而最底层的sched_group是跟CPU一一对应的，所以最终还是实现了CPU之间的平衡。

负载均衡比较好的3篇文章：
https://mp.weixin.qq.com/s/pLAFcD23qBP4iZxvek7xlg
https://mp.weixin.qq.com/s/-04fc_d0FNqxE6_asXFgpw
https://blog.csdn.net/feelabclihu/article/details/106435849?utm_medium=distribute.pc_relevant.none-task-blog-searchFromBaidu-5.channel_param&depth_1-utm_source=distribute.pc_relevant.none-task-blog-searchFromBaidu-5.channel_param
2、什么是均衡

		对于负载均衡而言，并不是把整个系统的负载平均的分配到系统中的各个CPU上。实际上，我们还是必须要考虑系统中各个CPU的算力，
让CPU获得和其算力匹配的负载。例如在一个6个小核+2个大核的系统中，整个系统如果有800的负载，那么每个CPU上分配100的负载其实是
不均衡的，因为大核CPU可以提供更强的算力。

		什么是CPU算力（capacity），所谓算力就是描述CPU的能够提供的计算能力。在同样的频率下，一个微架构是A77的CPU显然算力要大于A57的CPU。
如果CPU的微架构都是一样的，那么一个最大频率是2.2GHz的CPU算力肯定是大于最大频率是1.1GHz的CPU。因此，确定了微架构和最大频率，一个CPU的
算力就基本确定了。Cpufreq系统会根据当前的CPU util来调节CPU当前的运行频率，但这并不能改变CPU算力。只有当CPU最大运行频率发生变化的时候
（例如触发温控，限制了该CPU的最大频率），CPU的算力才会随之变化。
		CFS任务均衡中使用的CPU算力其实一个不断变化的值，需要经常更新。为了让CPU算力和任务负载可以对比，实际上我们采用了归一化的方式，
即系统中处理能力最强的CPU运行在最高频率的算力是1024，其他的CPU算力根据微架构和运行频率响应的调整其算力。

为了更好的进行CFS任务的均衡，系统需要跟踪任务负载和CPU负载。跟踪任务负载是主要有两个原因：

（1）判断该任务是否适合当前CPU算力。

（2）如果判定需要均衡，那么需要在CPU之间迁移多少的任务才能达到平衡？有了任务负载跟踪模块，这个问题就比较好回答了。

		有了上面描述的基础设施，那么什么时候进行负载均衡呢？这主要和调度事件相关，当发生任务唤醒、任务创建、tick到来等调度事件的时候，
我们可以检查当前系统的不均衡情况，并酌情进行任务迁移，以便让系统负载处于平衡状态。

		我们还定义了不均衡的门限值，也就是说domain的group之间如果有较小的不均衡，我们也是可以允许的，超过了门限值才发起负载均衡的操作。
		
1、整体的场景描述

在linux内核中，为了让任务均衡的分布在系统的所有CPU上，我们主要考虑下面三个场景：

（1）负载均衡（load balance）。通过搬移cpu runqueue上的任务，让各个CPU上的负载匹配CPU算力。

（2）任务放置（task placement）。当阻塞的任务被唤醒的时候，确定该任务应该放置在那个CPU上执行。

（3）主动均衡（active upmigration）。当一个低算力CPU的runqueue中出现misfit task的时候，如果该任务持续执行，那么负载均衡无能为力，
		因为它只负责迁移runnable状态的任务。这种场景下，active upmigration可以把当前正在运行的misfit task向上迁移到算力更高的CPU上去。
		
		
第二篇：
内核中，task placement场景发生在以下三种情况：

（1）进程通过fork创建子进程；

（2）进程通过sched_exec开始执行；

（3）阻塞的进程被唤醒。





trigger_load_balance中，1分钟触发一次软终端做一次负载均衡。

负载均衡 不同domain的创建：
rest_init
	kernel_init
		kernel_init_freeable();
			sched_init_smp();
				init_sched_domains(cpu_active_mask);
					build_sched_domains(doms_cur[0], NULL);
						for_each_sd_topology(tl) 
							build_sched_domain(tl, cpu_map, attr, sd, i);
								*sd = sd_init(tl, cpu)
负载均衡函数：
run_rebalance_domains
	