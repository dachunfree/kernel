
“对称多处理（Symmetric MultiProcessing， 简称 SMP）” 系统的优点在于，所有的处理器平等地享有对内存的访问权利。
但是这么做容易导致整体上系统处理效率的降低。共享数据需要互斥以避免并发访问；这意味着需要提供相应的锁机制以及
伴随而来的性能瓶颈问题。即使不存在对锁的争用，仅仅是因为在处理器之间对 “缓存行（cache line）” 数据进行移动就
足以对性能造成影响（译者注，这里指的是由于多个处理器的缓存行中存放了同一个全局变量的副本，任何一个处理器对本
地缓存行中该变量的修改都会导致其他处理器的缓存行中副本的失效，为此必须实现对该场景下缓存行中数据副本的同步刷新操作）。
所以提高 SMP 系统性能的关键举措就是要最大限度地减少数据共享，正因为如此，为了解决 “扩展性（scalability）” 的问题，
内核中引入了大量的 Per-CPU 变量（译者注，Per-CPU 变量是自 2.6 以来内核引入的一个有趣的特性，主要是为了解决 SMP 系统上
的变量访问问题。当建立一个 Per-CPU 变量时，系统中的每个处理器都会拥有该变量的特有副本。因为每个处理器在其自己的副本上工作，
所以对 Per-CPU 变量的访问不需要加锁；Per-CPU 变量还可以保存在对应的处理器的高速缓存中，这样，在频繁更新时可以获得更好的性能。
如果需要更详细的说明请参考 LWN 上对 Per-CPU 的介绍）。

Linux 内核中的 Per-CPU 变量本质上是一个数组，数组的每个元素对应一个处理器。每个处理器都使用自己的变量副本；这么做可以在无锁的
情况下完成对数据的访问操作，并且不用担心 “缓存行” 的 “bouncing” 问题（译者注，有关 “bouncing” 问题，参考 网上的一段解释）。例如，
一些 slab 分配器为每个处理器维护一份自己的（Per-CPU）空闲对象以及内存页的链表；由于不存在其他处理器的并发访问，我们可以在无需加锁
的情况下实现快速的分配和释放。如果没有这些 Per-CPU 类型的链表，随着处理器数量的增长，共享内存变量的 bouncing 问题就会愈发突出，自然
造成整体内存访问效率变差，扩展性降低。

但是，对 Per-CPU 数据的安全访问需要遵循一些限制前提：首先，访问数据的线程不能被抢占；其次，访问数据的线程在操作 Per-CPU 变量期间也不可
以被迁移到其他处理器上去。如果线程被抢占，替换它的线程可能会尝试访问相同的变量；类似的原因，如果线程在访问过程中被迁移到另一个处理器上
也可能会导致类似的混乱。为了避免这些风险，通常需要在访问 Per-CPU 变量的代码段的前后分别调用 get_cpu_var() 和 put_cpu_var()；对 get_cpu_var()
 的调用，除了返回当前处理器对应的变量的地址外（译者注，即下文所谓的获得对 Per-CPU 变量的引用），还会禁用抢占。因此，
 一个（调用了 get_cpu_var() 的）任务在（通过调用对应的 put_cpu_var() 函数）释放该引用之前，是不会被调度出当前处理器的
 （译者注，即其他任务也无法使用该处理器），从而保证了对 Per-CPU 变量访问的原子性。
 
问题：
我一直不明白per-CPU的意义。它是为了解决CPU之间同步问题引入的，但到底是怎么解决的呢？ 
每个CPU都有自己的副本，也没有讲副本和原始变量之间的同步。如果不需要同步，那每个CPU的副本不相当于四个变量了？
如果需要同步，那不跟设计per-cpu之前一样的问题？ 

答案：
1.统计计数：
SLUB 分配器在每个 CPU 上都维护一个 pageset 链表，然后每个 CPU 都会进行自己的 SLUB 分配，这时就需要 percpu 
变量来统计各自 cpu 上页分配的数量，最后 SLUB 再将所有 CPU 的页分配数加起来就知道现在系统到底分配了多少页。
PER-CPU 变量的应用场景就是每个 CPU 使用自己的副本，最后系统将所有副本统计起来，就可以知道某个数据的使用情况了

2.减少全局锁竞争。
每个cpu维护自己的链表即可。

static void tasklet_action(struct softirq_action *a)
	/*per-cpu 链表.从本cpu的tasklet链表中取出全部的tasklet，保存在list这个临时变量中，
	同时重新初始化本cpu的tasklet链表，使该链表为空。由于bottom half是开中断执行的，
	因此在操作tasklet链表的时候需要使用关中断保护
	*/
	list = __this_cpu_read(tasklet_vec.head);
	__this_cpu_write(tasklet_vec.head, NULL);
	__this_cpu_write(tasklet_vec.tail, this_cpu_ptr(&tasklet_vec.head));
