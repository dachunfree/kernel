		https://zhuanlan.zhihu.com/p/163482863?utm_source=wechat_session&utm_medium=social&utm_oi=730167171128233984
		在Linux SMP（对称多处理器）环境下，每个CPU对应一个run_queue（可执行队列）。如果一个进程处于TASK_RUNNING状态（可执行状态），
则它会被加入到其中一个run_queue（且同一时刻仅会被加入到一个run_queue），以便让调度程序安排它在这个run_queue对应的CPU上面运行。
	  一个CPU对应一个run_queue这样的设计，其好处是：
1、一个持续处于TASK_RUNNING状态的进程总是趋于在同一个CPU上面运行（其间，这个进程可能被抢占、然后又被调度），这有利于进程的数据被
	CPU所缓存，提高运行效率；
2、各个CPU上的调度程序只访问自己的run_queue，避免了竞争；
load_balance所需要做的事情就是，在一定的时机，通过将进程从一个run_queue迁移到另一个run_queue，来保持CPU之间的负载均衡。

普通进程的负载均衡在以下情况下会被触发：
1、当前进程离开TASK_RUNNING状态（进入睡眠或退出），而对应的run_queue中已无进程可用时。这时触发负载均衡，试图从别的run_queue
	中pull一个进程过来运行；
2、每隔一定的时间，启动负载均衡过程，试图发现并解决系统中不均衡；

load_balance的过程大致如下：
1、找出最繁忙的一个run_queue；
2、如果找到的run_queue比本地run_queue繁忙，且本地run_queue的繁忙程度低于平均水平，那么迁移几个进程过来，使两个run_queue的
		load接近平均水平。反之则什么都不做；

		在比较两个run_queue繁忙程度的问题上，其实是很有讲究的。这个地方很容易想当然地理解为：把run_queue中所有进程的load加起来，比较一下就OK了。
而实际上，需要比较的往往并不是实时的load。这就好比我们用top命令查看CPU占用率一样，top命令默认1秒刷新一次，每次刷新你将看到这1秒内所有进
程各自对CPU的占用情况。这里的占用率是个统计值，假设有一个进程在这1秒内持续运行了100毫秒，那么我们认为它占用了10%的CPU。如果把1秒刷新一次
改成1毫秒刷新一次呢？那么我们将有90%的机率看到这个进程占用0%的CPU、10%的机率占用100%的CPU。而无论是0%、还是100%，都不是这个进程真实的CPU
占用率的体现。必须把一段时间以内的CPU占用率综合起来看，才能得到我们需要的那个值。run_queue的load值也是这样。有些进程可能频繁地在
TASK_RUNNING和非TASK_RUNNING状态之间变换，导致run_queue的load值不断抖动。光看某一时刻的load值，我们是体会不到run_queue的负载情况的，
必须将一段时间内的load值综合起来看才行。于是，run_queue结构中维护了一个保存load值的数组：
unsigned long cpu_load[CPU_LOAD_IDX_MAX] （目前CPU_LOAD_IDX_MAX值为5）
每个CPU上，每个tick的时钟中断会调用到update_cpu_load函数，来更新该CPU所对应的run_queue的cpu_load值。
		cpu_load[i] = old_load + (new_load - old_load) / 2^i。i值越大，cpu_load[i]受load的实时值的影响越小，代表着越长时间内的平均负载情况。
而cpu_load[0]就是实时的load。
		尽管我们需要的是一段时间内的综合的负载情况，但是，为什么不是保存一个最合适的统计值，而要保存这么多的值呢？这是为了便于在不同场景下选择
不同的load。如果希望进行进程迁移，那么应该选择较小的i值，因为此时的cpu_load[i]抖动比较大，容易发现不均衡；反之，如果希望保持稳定，那么应该
选择较大的i值。
		什么时候倾向于进行迁移、什么时候又倾向于保持稳定呢？这要从两个维度来看：
		第一个维度，是当前CPU的状态。这里会考虑三种CPU状态：
1、CPU刚进入IDLE（比如说CPU上唯一的TASK_RUNNING状态的进程睡眠去了），这时候是很渴望马上弄一个进程过来运行的，应该选择较小的i值；
2、CPU处于IDLE，这时候还是很渴望弄一个进程过来运行的，但是可能已经尝试过几次都无果了，故选择略大一点的i值；
3、CPU非IDLE，有进程正在运行，这时候就不太希望进程迁移了，会选择较大的i值；
		第二个维度，是CPU的亲缘性。离得越近的CPU，进程迁移所造成的缓存失效的影响越小，应该选择较小的i值。比如两个CPU是同一物理CPU的同一
核心通过SMT（超线程技术）虚拟出来的，那么它们的缓存大部分是共享的。进程在它们之间迁移代价较小。反之则应该选择较大的i值。
（后面将会看到linux通过调度域来管理CPU的亲缘性）。

调度域

前面已经多次提到了调度域（sched_domain）。在复杂的SMP系统中，为了描述CPU与CPU之间的亲缘关系，引入了调度域。

两个CPU之间的亲缘关系主要有以下几种：

1、超线程。超线程CPU是一个可以“同时”执行几个线程的CPU。就像操作系统通过进程调度能够让多个进程“同时”在一个CPU上运行一样，超线程
	 CPU也是通过这样的分时复用技术来实现几个线程的“同时”执行的。这样做之所以能够提高执行效率，是因为CPU的速度比内存速度快很多
	 （一个数量级以上）。如果cache不能命中，CPU在等待内存的时间内将无事可做，可以切换到其他线程去执行。这样的多个线程对于操作系统
	 来说就相当于多个CPU，它们共享着大部分的cache，非常之亲近；

2、同一物理CPU上的不同核心。现在的多核CPU大多属于这种情况，每个CPU核心都有独立执行程序的能力，而它们之间也会共享着一些cache；

3、同一NUMA结点上的CPU；

4、不同NUMA结点上的CPU；

普通进程的load_balance第一步是需要找出一个最繁忙的CPU，实际上这是通过两个步骤来实现的：

1、找出sched_domain下最繁忙的一个sched_group（组内的CPU对应的run_queue的load之和最高）；

2、从该sched_group下找出最繁忙的CPU
可见，load_balance实际上是实现了对应sched_domain下的sched_group之间的平衡。较高层次的sched_domain包含了很多CPU，但是在这个
sched_domain上的load_balance并不直接解决这些CPU之间的负载均衡，而只是解决sched_group之间的平衡（这又是load_balance的一大简化）。
而最底层的sched_group是跟CPU一一对应的，所以最终还是实现了CPU之间的平衡。
